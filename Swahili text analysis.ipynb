{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b34b4d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Swahili Phrases Counting\n",
    "In this assignment, instead of calculating the word count as we did, we calculate count of phrases. \n",
    "In **Natural Language Processing(NLP)**, when we combine 2 or more words to make phrases, theye are called ```n-grams```. For example, a 2-gram is a two word term. \n",
    "\n",
    "## About the dataset\n",
    "We will use a Swahili news articles datast from [Zindi](https://zindi.africa/competitions/swahili-news-classification/data). We will get both the training and test dataset since we are only interested in the articles content. I already combined the train and text dataset and you can download it from our [course dataset directory].(https://drive.google.com/file/d/1iamzfWtojlYmbL3aMEFu9vOanZkHWfZN/view?usp=sharing)\n",
    "\n",
    "## Task decription\n",
    "In the classical word counting tasks, we set to generate word frequencies. However, in this task, we will write a function which generates frequencies \n",
    "on ```n-grams``` where ```n = 1, 2, 3 ....k```. \n",
    "\n",
    "## Tools\n",
    "For Spark, refer to documentation for Spark RDD and the intro-tospark-APIs notebook. We will also use the following packages:\n",
    "- [NLTK](https://www.nltk.org). A Python NLP library. Please install it.\n",
    "- [Googletranslate](https://pypi.org/project/googletrans/). A Python package for Google translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d5d8d-0607-46ca-903a-76f2f82e8c48",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python setup\n",
    "Import all the Python packages we need below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1d9b8d-59e7-400a-b427-701780ce2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the Python packages we need below\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "import nltk\n",
    "from googletrans import Translator\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3677673",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf01fa1-c3cb-4cd0-afaf-125459dc563b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inputs setup\n",
    "Lets provide paths to input files we will use. \n",
    "Its a good practice to create these as global variables. Also, use Python module ```Path``` from pathlib to manage file paths. If using ```Path``` is too hard for you, you can use abolute-paths to files and folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa670af-ffd2-4b5c-9c6d-3ef9edf713ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altenatively, you can put a full-path to wheree your data is located like below\n",
    "# DATA_DIR = Path(full-path-folder-where-you-are-keeping-data)\n",
    "DATA_DIR = Path().cwd().parents[0].joinpath(\"rD7R72ki9_Assignment_2_BDA\")\n",
    "\n",
    "# Swahili news\n",
    "SW_NEWS = DATA_DIR.joinpath('sw_short_articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b4da92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/GLC/Desktop/AIMS/REVIEW Course/BLOCK 4/Big Data Analysis/rD7R72ki9_Assignment_2_BDA/sw_short_articles.txt')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SW_NEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7074887-9caf-47f0-9c4b-ea2a41a8f83c",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed77b8a4-1f61-48c6-b96b-ab82c5fa38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_ngrams_phrase(text, n=3):\n",
    "    \"\"\"\n",
    "    Takes a text/article, splits into sentences and then creates n-gram tokens and returns them as phrases\n",
    "    Args:\n",
    "    text -- the text being processed\n",
    "    n -- whether to create a phrase with 1,2 3 or more words\n",
    "    \"\"\"\n",
    "    # Remove trailing spaces from the text\n",
    "    text =text.strip()\n",
    "    \n",
    "    # Split the text into sentences using NLTK function \n",
    "    # sent_tokenize()\n",
    "    # convert the text into lower case when you this\n",
    "    sentences = nltk.sent_tokenize(text.lower())\n",
    "    \n",
    "    # create a list to hold all phrases from this text string\n",
    "    phrases = []\n",
    "    \n",
    "    # Loop through all sentences and do the following\n",
    "    # 1. use NLTK ngrams() function to split each sentence into\n",
    "    # n-grams\n",
    "    # 2. For each n-gram, convert it into a list and create a string \n",
    "    # where words are separated by space\n",
    "    # 3. Add that string to the list above\n",
    "    # ~ 5 lines of code\n",
    "    for i in sentences:\n",
    "        ngrm=nltk.ngrams(i.split(), n)\n",
    "        list_ngrm=list(ngrm)\n",
    "        strg=[' '.join(i) for i in list_ngrm ]\n",
    "        phrases.append(strg)\n",
    "   \n",
    "    # Return all phrases as a single string by concatening them with comma as a separator\n",
    "    # using string method like this: \",\".join(list)     \",\".join(phrases) \n",
    "    return [','.join(j) for j in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64098246-c8d2-4bcd-b295-3789d313e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_clean_up(phrase, phrase_min_len=3): #10\n",
    "    \"\"\"\n",
    "    Provide a label 1-to remove this row because phrase is too short or contains just numbers or special chacters or 0 when its okay.\n",
    "    Args:\n",
    "    phrase -- Target phrase\n",
    "    phrase_min_len --  Keep only phrases with this length or longer\n",
    "    \n",
    "    returns: 1\n",
    "    \"\"\"\n",
    "    # Replace pass with your code\n",
    "    if len(phrase) < phrase_min_len or phrase.isalnum() or phrase.isnumeric():\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f83b9c4-66b4-40a3-8000-40a526fd12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows(line):\n",
    "    \"\"\"\n",
    "    Used to drop rows in the file we dont need. For example, first row is header, we skip it.\n",
    "    We use this function with the Spark RDD filter function. As such, we need to return \n",
    "    True or False based on things we need.\n",
    "    Args:\n",
    "    line -- a single line of text in the target RDD\n",
    "    \"\"\"\n",
    "    # 1. Split line using the delimiter used\n",
    "    # 2. Retrieve the text which contains the article\n",
    "    # 3. Skip the header \n",
    "    # 4. Skip very short texts (e.g., those with length < 100)\n",
    "    # You can wrap your code in try-except-block\n",
    "    # so that if things go wromg, your code can still run\n",
    "    # replace pass with your code\n",
    "    try:\n",
    "        col_items = line.split(\"\\t\")\n",
    "        if col_items[2] != \"content\" and len(line)>100:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cadb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    \"\"\"\n",
    "    Split the line which has a single string\n",
    "    into a list where each element represents a\n",
    "    column\n",
    "    \"\"\"\n",
    "    # Split the line\n",
    "    col_items = line.split(\"\\t\")\n",
    "    \n",
    "    # We need only the colum with the text\n",
    "    col_items = col_items[2]\n",
    "    \n",
    "    try:\n",
    "        return str(col_items)\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac1f24-8776-41d4-b929-afe9550ddc24",
   "metadata": {},
   "source": [
    "### Creating a word count function\n",
    "In the function ```run_phrase_count_on_rdd()``` below, \n",
    "the main ingredient is the piece of code where you apply several transformations on the RDD to generate the phrase counts. The main steps you will follow include the following (not necessarily in this same order):\n",
    "- **filter**. You need to filter the RDD so that you skip the header and other bad rows\n",
    "- **map**. You will need more than 1 ```map``` operations to apply functions to each line of the RDD to split it \n",
    "into columns and convert each line of text into n-grams using NLTK functions. \n",
    "- **flatMap**. At some point, you will need to do flatMap to flatten the RDD in preparation for reduce. Please refer to ```introducing-spark-APIs``` notebook.\n",
    "- **reduce or reduceByKey**. You decide whether to use ```reduce``` or ```reduceByKey```. Please refer to Spark RDD API documentation and ```introducing-spark-APIs``` notebook for details.\n",
    "\n",
    "#### Hints\n",
    "1. Before you run this function, make sure you have tested your code to compute phrase count outside the function.\n",
    "2. When computing word counts, although you can chain the operators together, its recommended to compute one transformation at a time and verify that its working before chaining many operations together.\n",
    "3. Note that you will pass the ```function tokenize_into_ngrams_phrase(text, n=3)``` above to one of the ```map``` or ```flatMap``` operations explaine above. Please test and esnure that the ```tokenize_into_ngrams_phrase(text, n=3)``` is working before you use it in map.\n",
    "4. In order to explore the data and test things, feel free to load the data into a pandas DataFrame.\n",
    "5. Expected number of columns in the output CSV file is 3: ```phrase, count and phrase_en``` and grading process will check this.\n",
    "\n",
    "#### Converting texts into n-grams\n",
    "Again, an n-gram is just a phrase with n-words. In this exercise, I recommend we use NLTK to generate n-grams without much effort. For this, you will need to import module/function ```ngrams``` from NLTK and another function ```sent_tokenize``` from \n",
    "```nltk.tokenize``` module. Please explore the documentation to understand how to use these functions. \n",
    "\n",
    "#### Cleaning up phrases \n",
    "During tokenization, we are not performing any preprocessign as required in NLP. As such, we end up with nonsensical and noisy phrases such as ```0000``` or ```blank``` string. We need to remove these from our results. Please feel free to use any approach to remove these. \n",
    "However, I have added a function called ``` quick_clean_up()``` which you can use/complete utilizing phrase length to do this clean-up.\n",
    "\n",
    "#### Translating to English\n",
    "The phrases are in Swahili.  However, your output will be a CSV file with only phrases which we translate to English. We will use ```Google translate``` for this. Please feel free to use another translaiton package if it works better. For Google-translate, the translate is quite slow, so instead of translating all phrases, we just translate the ```top-k``` occuring phrases (after clean-up). For example, translating 500 phrases is taking around 2-3 minutes for me. So, when testing set your k at some small number. Please learn how to use this package from its documentation page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32baa35-e80d-4439-a7e9-06c4220622f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_phrase_count_on_rdd(phrase_length, out_csv, num_to_translate=100, phrase_min_len = 12):\n",
    "    \"\"\"\n",
    "    Generate phrase count from RDD and return dataframe with \n",
    "    phrase counts and their English translations\n",
    "    Args:\n",
    "    phrase_length --  whether its 3-grams, 2-gram or 1-gram\n",
    "    out_csv --  path to CSV to save results\n",
    "    num_to_translate -- because translation takes too long, we translate only top-k phrases provided by this param\n",
    "    phrase_min_len -- when cleaning up phrases, we can use length of phrase to decide which phrases to throw out\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # =======================\n",
    "    # LOAD DATA INTO RDD\n",
    "    # ======================\n",
    "    spark = SparkSession.builder.appName(\"intro\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text_rdd = sc.textFile(str(SW_NEWS))\n",
    "    \n",
    "    # =======================\n",
    "    # RUN WORD COUNT\n",
    "    # ======================\n",
    "    # Please refer to explnation above on how to compute phrase counts\n",
    "    rdd_phrase_cnt=(text_rdd.filter(filter_rows)\n",
    "                    .map(split_line)\n",
    "                    .flatMap(tokenize_into_ngrams_phrase)\n",
    "                    .flatMap(lambda x: x.split(','))\n",
    "                    .map(lambda line: re.sub(r'[^\\w\\s]', '', line) )\n",
    "                    .map(lambda line: line.strip())\n",
    "                    .filter(lambda line: quick_clean_up(line) )\n",
    "                    .map(lambda x: (x,1))\n",
    "                    .reduceByKey(lambda x, y: x + y))\n",
    "    \n",
    "    # =======================\n",
    "    # CONVERT INTO DATAFRAME\n",
    "    # ======================\n",
    "    # Use collect() to get a list of phrases and their counts\n",
    "    # and then create a Pandas DataFrame\n",
    "    # please call column names 'phrase' and 'count'\n",
    "    # ~ 2 lines of code\n",
    "    res = rdd_phrase_cnt.collect()\n",
    "    df = pd.DataFrame(res, columns=['phrase','count'])\n",
    "    \n",
    "    # =======================\n",
    "    # QUICK CLEAN-UP\n",
    "    # ======================\n",
    "    # Remove noisy phrases such as \"\" (blank), '000' numbers  and others\n",
    "    # Refer to explanation above for hints\n",
    "    # ~ 1-5 lines of code depending on how you choose to solve this problem    \n",
    "    \n",
    "    df=df.sort_values(by=\"count\",ascending=False).reset_index(drop=True)\n",
    "    df=df.iloc[0:num_to_translate,:]\n",
    "    \n",
    "    # ============================================================\n",
    "    # TRANSLATE TOP K-PHRASES AND SAVE THOSE PHRASES TO CSV FILE\n",
    "    # ============================================================\n",
    "    # translate to English only top- phrases and save the resulting\n",
    "    # dataframe to CSV with the English version of the phrase in\n",
    "    # a column named: 'phrase_en'\n",
    "    # ~ 4-6 lines of code\n",
    "    df=df.iloc[0:num_to_translate,:]\n",
    "    translator=Translator()\n",
    "    df['phrase_en']=[translator.translate(df.iloc[i,0], dest='en').text for i in range(df.shape[0])]\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    \n",
    "    print('Successfully saved CSV file for top-{} phrases in the Swahili news corpus'.format( num_to_translate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23210349-3ece-4af0-8ba3-609bd251da00",
   "metadata": {},
   "source": [
    "## Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d251e8-024a-4d7e-9f27-fc4cb65afb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved CSV file for top-100 phrases in the Swahili news corpus\n",
      "Translating 100 phrases took 1 minutes\n"
     ]
    }
   ],
   "source": [
    "# Initialize spark and create spark-context\n",
    "spark = SparkSession.builder.appName(\"intro\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Setup parameters for phrase count\n",
    "# and yu can use the time variables\n",
    "# to profile your code\n",
    "\n",
    "# Record start time using datetime.now()\n",
    "start = time.time()\n",
    "\n",
    "# phrase length, your script should work with \n",
    "# any phrase length up t0 5\n",
    "n = 5\n",
    "\n",
    "# How many phrases to translate to english\n",
    "# you can start with 100\n",
    "translate_to_en = 100\n",
    "\n",
    "# When cleaning up, whats the shortest phrase you\n",
    "# think is reasonable. For example, it can be n-gram * 4\n",
    "min_phrase_len = 12\n",
    "\n",
    "# path tp CSV to save top-k translated phrases\n",
    "output_path = 'translated_phrases.csv'\n",
    "\n",
    "# Finaly, run the run_phrase_count_on_rdd() function\n",
    "run_phrase_count_on_rdd(n, output_path, translate_to_en, 3)\n",
    "end = time.time()\n",
    "\n",
    "# Minutes taken to run the whole thing\n",
    "time_taken = (end-start)/60\n",
    "print('Translating {} phrases took {} minutes'.format(translate_to_en , int(time_taken)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ea9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
